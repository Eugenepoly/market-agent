"""Monitor Agent - monitors VIP accounts and generates alerts."""

import os
import json
from datetime import datetime
from typing import Any, List, Optional

from core.base_agent import BaseAgent
from core.state import WorkflowContext
from collectors.social import XCollector, TruthCollector
from watchlist import VIP_ACCOUNTS, ALERT_KEYWORDS


class MonitorAgent(BaseAgent):
    """Agent for monitoring VIP social accounts and detecting market-moving content."""

    name = "monitor_agent"
    requires_approval = False

    def __init__(self, data_dir: str = "./data"):
        """Initialize the monitor agent."""
        super().__init__()
        self.data_dir = data_dir
        self.x_collector = XCollector(data_dir)
        self.truth_collector = TruthCollector(data_dir)

    def get_prompt(self, context: WorkflowContext) -> str:
        """Generate analysis prompt based on collected posts."""
        collected_data = context.data.get("collected_posts", {})

        posts_text = self._format_posts_for_prompt(collected_data)
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M")

        return f"""
### è§’è‰²ï¼šç¤¾äº¤åª’ä½“å¸‚åœºä¿¡å·åˆ†æžå¸ˆ

### ä»»åŠ¡
åˆ†æžä»¥ä¸‹æ¥è‡ªé‡è¦äººç‰©ï¼ˆå¤§Vï¼‰çš„æœ€æ–°ç¤¾äº¤åª’ä½“å¸–å­ï¼Œè¯†åˆ«å¯èƒ½å½±å“å¸‚åœºçš„ä¿¡å·ã€‚

### æ”¶é›†åˆ°çš„å¸–å­
{posts_text}

### åˆ†æžè¦æ±‚

1. **å¸‚åœºå½±å“è¯„ä¼°**
   å¯¹æ¯æ¡é‡è¦å¸–å­è¯„ä¼°å…¶å¸‚åœºå½±å“ï¼š
   - ðŸ”´ é«˜å½±å“ï¼šå¯èƒ½ç›´æŽ¥å½±å“èµ„äº§ä»·æ ¼ï¼ˆå¦‚æ”¿ç­–å£°æ˜Žã€æ”¶è´­æ¶ˆæ¯ï¼‰
   - ðŸŸ¡ ä¸­å½±å“ï¼šå€¼å¾—å…³æ³¨ä½†å½±å“æœ‰é™
   - ðŸŸ¢ ä½Žå½±å“ï¼šæ—¥å¸¸å†…å®¹ï¼Œæ— æ˜Žæ˜¾å¸‚åœºå½±å“

2. **å…³é”®ä¿¡æ¯æå–**
   - æåŠçš„å…·ä½“èµ„äº§/å…¬å¸/åŠ å¯†è´§å¸
   - æ”¿ç­–æ–¹å‘æš—ç¤º
   - å¸‚åœºæƒ…ç»ªä¿¡å·ï¼ˆçœ‹å¤š/çœ‹ç©º/ä¸­ç«‹ï¼‰

3. **è·¨è´¦å·å…³è”**
   - ä¸åŒå¤§Vä¹‹é—´æ˜¯å¦æœ‰å‘¼åº”/å†²çªçš„è§‚ç‚¹
   - æ˜¯å¦å­˜åœ¨ä¿¡æ¯å¥—åˆ©æœºä¼š

4. **è¡ŒåŠ¨å»ºè®®**
   åŸºäºŽä»¥ä¸Šåˆ†æžï¼Œç»™å‡ºï¼š
   - éœ€è¦å¯†åˆ‡å…³æ³¨çš„èµ„äº§
   - æ½œåœ¨çš„äº¤æ˜“æœºä¼šæˆ–é£Žé™©

### è¾“å‡ºæ ¼å¼

# VIP ç¤¾äº¤ç›‘æŽ§æŠ¥å‘Š [{current_time}]

## ðŸ“¢ é‡è¦å¸–å­æ‘˜è¦
[æŒ‰å½±å“ç¨‹åº¦æŽ’åºåˆ—å‡º]

## ðŸŽ¯ å¸‚åœºä¿¡å·
[æå–çš„å…³é”®å¸‚åœºä¿¡å·]

## âš¡ è¡ŒåŠ¨å»ºè®®
[å…·ä½“å»ºè®®]

## ðŸ“Š æƒ…ç»ªæ€»è§ˆ
[æ€»ä½“å¸‚åœºæƒ…ç»ªåˆ¤æ–­]
"""

    def _format_posts_for_prompt(self, collected_data: dict) -> str:
        """Format collected posts for the analysis prompt."""
        sections = []

        for source, posts in collected_data.items():
            if not posts:
                continue

            section = f"\n### {source.upper()}\n"
            for post in posts:
                handle = post.get("handle", "unknown")
                content = post.get("content", "")
                timestamp = post.get("timestamp", "")

                # Truncate very long content
                if len(content) > 500:
                    content = content[:500] + "..."

                section += f"\n**@{handle}** ({timestamp}):\n{content}\n"

            sections.append(section)

        return "\n".join(sections) if sections else "æ²¡æœ‰æ”¶é›†åˆ°æ–°å¸–å­"

    def collect_all(self) -> dict:
        """Collect posts from all configured sources."""
        collected = {}

        # Collect from X
        x_result = self.x_collector.collect()
        if x_result.success and x_result.data:
            collected["x"] = x_result.data
            self.x_collector.save_data(x_result, "social_posts")

        # Collect from Truth Social
        truth_result = self.truth_collector.collect()
        if truth_result.success and truth_result.data:
            collected["truth_social"] = truth_result.data
            self.truth_collector.save_data(truth_result, "social_posts")

        return collected

    def detect_keywords(self, posts: List[dict]) -> List[dict]:
        """Detect alert keywords in posts."""
        alerts = []

        for post in posts:
            content = post.get("content", "").lower()
            matched_keywords = []

            for category, keywords in ALERT_KEYWORDS.items():
                for keyword in keywords:
                    if keyword.lower() in content:
                        matched_keywords.append((category, keyword))

            if matched_keywords:
                alerts.append({
                    "post": post,
                    "matched_keywords": matched_keywords,
                    "alert_level": "high" if len(matched_keywords) >= 3 else "medium",
                })

        return alerts

    def run(self, context: WorkflowContext) -> Any:
        """Execute the monitor agent.

        Collects posts, detects keywords, and generates analysis.
        """
        from core.state import AgentResult

        try:
            # Step 1: Collect posts
            collected = self.collect_all()

            if not collected:
                return AgentResult(
                    agent_name=self.name,
                    success=True,
                    output={
                        "message": "No new posts collected",
                        "posts": {},
                        "alerts": [],
                    },
                )

            # Step 2: Detect keywords
            all_posts = []
            for source_posts in collected.values():
                all_posts.extend(source_posts)

            alerts = self.detect_keywords(all_posts)

            # Step 3: Add collected data to context for analysis
            context.data["collected_posts"] = collected

            # Step 4: Generate analysis using LLM
            prompt = self.get_prompt(context)

            from google.genai import types
            response = self.client.models.generate_content(
                model=self.config.model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    tools=[types.Tool(google_search=types.GoogleSearch())]
                ),
            )

            analysis = response.text

            # Save analysis as markdown
            filepath = self._save_analysis(analysis)

            return AgentResult(
                agent_name=self.name,
                success=True,
                output={
                    "analysis": analysis,
                    "filepath": filepath,
                    "posts_collected": len(all_posts),
                    "sources": list(collected.keys()),
                    "alerts": alerts,
                    "keyword_alerts_count": len(alerts),
                },
            )

        except Exception as e:
            return AgentResult(
                agent_name=self.name,
                success=False,
                error=str(e),
            )

    def run_quick_check(self) -> dict:
        """Run a quick check without full LLM analysis.

        Useful for hourly monitoring.
        """
        collected = self.collect_all()

        all_posts = []
        for source_posts in collected.values():
            all_posts.extend(source_posts)

        alerts = self.detect_keywords(all_posts)

        result = {
            "timestamp": datetime.utcnow().isoformat(),
            "posts_collected": len(all_posts),
            "sources": list(collected.keys()),
            "alerts": alerts,
            "high_priority_alerts": [a for a in alerts if a.get("alert_level") == "high"],
            "collected": collected,
        }

        # Save markdown summary
        self._save_posts_summary(result)

        return result

    def _save_posts_summary(self, result: dict) -> str:
        """Save collected posts as a markdown summary."""
        timestamp = datetime.now().strftime("%Y%m%d_%H")
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M")
        output_dir = os.path.join(self.data_dir, "monitor")
        os.makedirs(output_dir, exist_ok=True)

        # Build markdown content
        lines = [
            f"# VIP ç¤¾äº¤ç›‘æŽ§æ•°æ®æ‘˜è¦ [{current_time}]",
            "",
            "## é‡‡é›†ç»Ÿè®¡",
            f"- **å¸–å­æ€»æ•°**: {result.get('posts_collected', 0)}",
            f"- **æ•°æ®æ¥æº**: {', '.join(result.get('sources', []))}",
            f"- **é«˜ä¼˜å…ˆçº§å‘Šè­¦**: {len(result.get('high_priority_alerts', []))} æ¡",
            f"- **é‡‡é›†æ—¶é—´**: {result.get('timestamp', '')}",
            "",
        ]

        # High priority alerts
        high_alerts = result.get("high_priority_alerts", [])
        if high_alerts:
            lines.extend([
                "## ðŸ”´ é«˜ä¼˜å…ˆçº§å‘Šè­¦",
                "",
                "| è´¦å· | å†…å®¹æ‘˜è¦ | å…³é”®è¯ |",
                "|------|----------|--------|",
            ])
            for alert in high_alerts:
                post = alert.get("post", {})
                handle = post.get("handle", "unknown")
                content = post.get("content", "")[:80].replace("|", "\\|").replace("\n", " ")
                keywords = ", ".join([kw[1] for kw in alert.get("matched_keywords", [])])
                lines.append(f"| @{handle} | {content}... | {keywords} |")
            lines.append("")

        # All alerts
        all_alerts = result.get("alerts", [])
        medium_alerts = [a for a in all_alerts if a.get("alert_level") == "medium"]
        if medium_alerts:
            lines.extend([
                "## ðŸŸ¡ ä¸­ä¼˜å…ˆçº§å‘Šè­¦",
                "",
            ])
            for alert in medium_alerts[:5]:
                post = alert.get("post", {})
                handle = post.get("handle", "unknown")
                content = post.get("content", "")[:100].replace("\n", " ")
                keywords = ", ".join([kw[1] for kw in alert.get("matched_keywords", [])])
                lines.append(f"- **@{handle}**: {content}... (å…³é”®è¯: {keywords})")
            lines.append("")

        # Posts by source
        collected = result.get("collected", {})
        for source, posts in collected.items():
            if not posts:
                continue
            source_name = "X/Twitter" if source == "x" else "Truth Social"
            lines.extend([
                f"## {source_name} ({len(posts)} æ¡)",
                "",
            ])
            for post in posts[:10]:
                handle = post.get("handle", "unknown")
                content = post.get("content", "")[:150].replace("\n", " ")
                timestamp_str = post.get("timestamp", "")
                stats = post.get("stats", {})
                stats_str = ""
                if stats:
                    likes = stats.get("likes", 0)
                    retweets = stats.get("retweets", 0)
                    if likes or retweets:
                        stats_str = f" (â¤ï¸ {likes:,}, ðŸ” {retweets:,})"
                lines.append(f"- **@{handle}** ({timestamp_str}){stats_str}")
                lines.append(f"  > {content}...")
                lines.append("")

        # Save file
        filename = f"summary_{timestamp}.md"
        filepath = os.path.join(output_dir, filename)

        with open(filepath, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))

        # Cleanup old files
        self._cleanup_old_files(output_dir, "summary_", max_files=3)

        return filepath

    def _save_analysis(self, analysis: str) -> str:
        """Save analysis report as markdown file."""
        timestamp = datetime.now().strftime("%Y%m%d_%H")
        output_dir = os.path.join(self.data_dir, "monitor")
        os.makedirs(output_dir, exist_ok=True)

        filename = f"analysis_{timestamp}.md"
        filepath = os.path.join(output_dir, filename)

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(analysis)

        # Cleanup old files (keep last 3)
        self._cleanup_old_files(output_dir, "analysis_", max_files=3)

        return filepath

    def _cleanup_old_files(self, directory: str, prefix: str, max_files: int = 3):
        """Remove old files, keeping only the most recent ones."""
        try:
            files = [f for f in os.listdir(directory) if f.startswith(prefix) and f.endswith(".md")]
            if len(files) > max_files:
                files.sort()
                for filename in files[:-max_files]:
                    os.remove(os.path.join(directory, filename))
        except Exception:
            pass
